{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary python extensions#\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import io\n",
    "import csv\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all zipfiles to temporary folder#\n",
    "zip_files = os.listdir(\"Zipfiles/\")\n",
    "for item in os.listdir(\"Zipfiles/\"):\n",
    "    if item.endswith(\".zip\"):\n",
    "        with ZipFile(\"Zipfiles/\" + item, 'r') as zf:\n",
    "            zf.printdir()\n",
    "            zf.extractall(\"TempZipFolder/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out which files are semicolon delimited#\n",
    "delimiters = dict() \n",
    "for item in os.listdir(\"Zipfiles/\"):\n",
    "    if item.endswith(\".zip\"):\n",
    "        with ZipFile(\"Zipfiles/\" + item, 'r') as zf:\n",
    "            zipped_files = zf.namelist()\n",
    "            for file_name in zipped_files :\n",
    "                input_file = zf.open(file_name,'r')\n",
    "                input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "            \n",
    "                dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "            \n",
    "                delimiters[file_name] = dialect.delimiter\n",
    "            \n",
    "                #input_file.seek(0)\n",
    "            \n",
    "                print(\" \".join([\"It looks like\",\n",
    "                           file_name,\n",
    "                           \"has delimiter\",\n",
    "                           dialect.delimiter,\n",
    "                           \".\"]))\n",
    "\n",
    "                input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of semicolon_delimited files for import#\n",
    "semicolon_delimited = [k for k,v in delimiters.items() if v == ';']\n",
    "comma_delimited = [k for k,v in delimiters.items() if v == ',']\n",
    "print(semicolon_delimited[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which files have headers, make a list of those filenames#\n",
    "noheads = []\n",
    "for this_zf in zip_files :\n",
    "    with ZipFile(\"ZipFiles/\" + this_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files :\n",
    "            input_file = zf.open(file_name,'r')\n",
    "            input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "            \n",
    "            this_delimiter = delimiters[file_name]\n",
    "            \n",
    "            for line in input_file :\n",
    "                    noheads.append(file_name + ';' + str(\"datetime\" in line))\n",
    "                    break\n",
    "            input_file.close() # tidy up\n",
    "heads = []\n",
    "for obj in noheads:\n",
    "    if obj[-4:] == r'True':\n",
    "        heads.append(obj.split(';')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas, clean files of commas, semicolons, and line breaks, export all back to csv with same headers and commas#\n",
    "for obj in os.listdir(\"CleanFiles/\"):\n",
    "    if obj.endswith(\".csv\"):\n",
    "        os.remove(\"CleanFiles/\" + obj)\n",
    "for part in os.listdir(\"TempZipFolder/\"):\n",
    "    headers = [\"datetime\",\"register_no\",\"emp_no\",\"trans_no\",\"upc\",\"description\",\"trans_type\",\"trans_subtype\",\"trans_status\",\"department\",\"quantity\",\"Scale\",\"cost\",\"unitPrice\",\"total\",\"regPrice\",\"altPrice\",\"tax\",\"taxexempt\",\"foodstamp\",\"wicable\",\"discount\",\"memDiscount\",\"discountable\",\"discounttype\",\"voided\",\"percentDiscount\",\"ItemQtty\",\"volDiscType\",\"volume\",\"VolSpecial\",\"mixMatch\",\"matched\",\"memType\",\"staff\",\"numflag\",\"itemstatus\",\"tenderstatus\",\"charflag\",\"varflag\",\"batchHeaderID\",\"local\",\"organic\",\"display\",\"receipt\",\"card_no\",\"store\",\"branch\",\"match_id\",\"trans_id\"]\n",
    "    for ele in semicolon_delimited:\n",
    "        if part == ele:\n",
    "            df = pd.read_csv(\"TempZipFolder/\" + part, sep = ';', header=0, names=headers, encoding = 'utf-8',low_memory=False)\n",
    "            print(part + 'has semicolons')\n",
    "            df = df.replace(r'\\n',\" \", regex=True)\n",
    "            df = df.replace(r'\\\\n',\" \", regex=True)\n",
    "            df = df.replace(r'\\N',\" \")\n",
    "            df = df.replace(r'\\\\N',\" \")\n",
    "            df = df.replace(r',',\" \", regex=True)\n",
    "            df = df.replace(r';',\" \", regex=True)\n",
    "            df.to_csv(r'TempFiles/clean' + part, sep = \",\", header = True, index = False)\n",
    "            print(\"generated clean file for {}\".format(part))\n",
    "            break\n",
    "    else:\n",
    "        df = pd.read_csv(\"TempZipFolder/\" + part, sep = ',', header=0, encoding = 'utf-8', names=headers,low_memory=False)\n",
    "        df = df.replace(r'\\n',\" \", regex=True)\n",
    "        df = df.replace(r'\\\\n',\" \", regex=True)\n",
    "        df = df.replace(r'\\N',\" \")\n",
    "        df = df.replace(r'\\\\N',\" \")\n",
    "        df = df.replace(r',',\" \", regex=True)\n",
    "        df = df.replace(r';',\" \", regex=True)\n",
    "        df.to_csv(r'TempFiles/clean' + part, sep = \",\", header = True, index = False)\n",
    "        print(\"generated clean file for {}\".format(part))\n",
    "        continue\n",
    "        print('clean file upload complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reimport as DataFrame, this time to make all datatypes the same and remove population of \"nan\" values#\n",
    "for part in os.listdir(\"TempFiles/\"):\n",
    "    for ele in os.listdir(\"CleanFiles/\"):\n",
    "        if part == ele:\n",
    "            break\n",
    "    else:\n",
    "        df = pd.read_csv(\"TempFiles/\" + part, sep = ',', header=0, names=headers, na_filter=False)\n",
    "        df = df.astype({\"datetime\":\"object\",\"register_no\":\"float\",\"emp_no\":\"float\",\"trans_no\":\"float\",\"upc\":\"string\",\"description\":\"string\",\"trans_type\":\"string\",\"trans_subtype\":\"string\",\"trans_status\":\"string\",\"department\":\"float\",\"quantity\":\"float\",\"Scale\":\"float\",\"cost\":\"float\",\"unitPrice\":\"float\",\"total\":\"float\",\"regPrice\":\"float\",\"altPrice\":\"float\",\"tax\":\"float\",\"taxexempt\":\"float\",\"foodstamp\":\"float\",\"wicable\":\"float\",\"discount\":\"float\",\"memDiscount\":\"float\",\"discountable\":\"float\",\"discounttype\":\"float\",\"voided\":\"float\",\"percentDiscount\":\"float\",\"ItemQtty\":\"float\",\"volDiscType\":\"float\",\"volume\":\"float\",\"VolSpecial\":\"float\",\"mixMatch\":\"float\",\"matched\":\"float\",\"memType\":\"bool\",\"staff\":\"bool\",\"numflag\":\"float\",\"itemstatus\":\"float\",\"tenderstatus\":\"float\",\"charflag\":\"string\",\"varflag\":\"float\",\"batchHeaderID\":\"bool\",\"local\":\"float\",\"organic\":\"float\",\"display\":\"bool\",\"receipt\":\"float\",\"card_no\":\"float\",\"store\":\"float\",\"branch\":\"float\",\"match_id\":\"float\",\"trans_id\":\"float\"},errors='ignore')\n",
    "        df.to_csv(r'CleanFiles/' + part, sep = \",\", header = True, index = False)\n",
    "        print(\"generated clean file for {}\".format(part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create credentials, service file path, client for GBQ# \n",
    "service_path = \"/Users/peterkirgis/Downloads/\"\n",
    "service_file = 'wedge-project-peterkirgis-1e7b528def57.json'\n",
    "gbq_proj_id = 'wedge-project-peterkirgis'  \n",
    "gbq_dataset_id = 'WedgeData2' \n",
    "\n",
    "private_key =service_path + service_file\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload cleaned files to GBQ with schema#\n",
    "for file in os.listdir('CleanFiles/'):\n",
    "    if file.endswith('.csv'):\n",
    "        tablename = file.split(\".\")[0]\n",
    "        table_full_name = \".\".join([gbq_proj_id,gbq_dataset_id,tablename])\n",
    "        print(table_full_name)\n",
    "    \n",
    "        def tbl_exists(client, table_ref):\n",
    "            from google.cloud.exceptions import NotFound\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                return True\n",
    "            except NotFound:\n",
    "                return False\n",
    "    \n",
    "        if not tbl_exists(client, table_full_name) :\n",
    "            table_ref = client.create_table(\n",
    "            table = table_full_name\n",
    "            )\n",
    "        else :\n",
    "            print(\"table {} already exists\".format(table_full_name))\n",
    "            continue\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "        job_config.schema_update_options = [\n",
    "        bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION # This allows us to modify the table. \n",
    "        ]\n",
    "        job_config.schema = [\n",
    "        bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        ]\n",
    "        job_config.source_format = bigquery.SourceFormat.CSV\n",
    "        job_config.skip_leading_rows = 1\n",
    "    \n",
    "        with open('CleanFiles/' + file, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            location=\"US\",  # Must match the destination dataset location.\n",
    "            job_config=job_config,\n",
    "            )  # API request\n",
    "        job.result()  # Waits for table load to complete.\n",
    "        print(\n",
    "        \"Loaded {} rows into {}:{}.\".format(\n",
    "            job.output_rows, 'WedgeData2', table_ref.table_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
